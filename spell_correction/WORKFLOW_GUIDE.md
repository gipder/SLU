# ê°œì„ ëœ ì›Œí¬í”Œë¡œìš° ê°€ì´ë“œ

## ì´ì „ ì›Œí¬í”Œë¡œìš° (3ë‹¨ê³„)
1. `extract_feature.py` - ê°œë³„ `.pt` íŒŒì¼ ìƒì„±
2. `create_tar_dataset.py` - tar ì•„ì¹´ì´ë¸Œë¡œ ì••ì¶•
3. `eval.py` or `train.py` - ëª¨ë¸ ì‹¤í–‰

## ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° (2ë‹¨ê³„)
1. `extract_feature.py` - ê°œë³„ `.pt` íŒŒì¼ ìƒì„±
2. `create_tar_dataset.py` (ê°œì„ ë¨) - tar ì•„ì¹´ì´ë¸Œ + ë©”íƒ€ë°ì´í„° ìºì‹œ ë™ì‹œ ìƒì„±

---

## ìƒì„¸ ì‚¬ìš©ë²•

### 1ë‹¨ê³„: í”¼ì²˜ ì¶”ì¶œ
```bash
cd spell_correction
python extract_feature.py
```

ê°œë³„ `.pt` íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:
- `./hubert_deberta_cache_retrial/train/.../*.pt`
- `./hubert_deberta_cache_retrial/eval_0/.../*.pt`
- `./hubert_deberta_cache_retrial/test_0/.../*.pt`

### 2ë‹¨ê³„: TAR ì•„ì¹´ì´ë¸Œ + ìºì‹œ ìƒì„± (ê°œì„ ë¨)

**Train ë°ì´í„°:**
```bash
python create_tar_dataset.py \
  --source_dir ./hubert_deberta_cache_retrial \
  --output_dir ./hubert_deberta_tar \
  --task train \
  --num_shards 4 \
  --tokenizer_model_name facebook/hubert-large-ls960-ft
```

**Eval ë°ì´í„°:**
```bash
python create_tar_dataset.py \
  --source_dir ./hubert_deberta_cache_retrial \
  --output_dir ./hubert_deberta_tar \
  --task eval_0 \
  --num_shards 2 \
  --tokenizer_model_name facebook/hubert-large-ls960-ft
```

**Test ë°ì´í„°:**
```bash
python create_tar_dataset.py \
  --source_dir ./hubert_deberta_cache_retrial \
  --output_dir ./hubert_deberta_tar \
  --task test_0 \
  --num_shards 2 \
  --tokenizer_model_name facebook/hubert-large-ls960-ft
```

### ê²°ê³¼ë¬¼

ê° taskë³„ë¡œ ìƒì„±ë˜ëŠ” íŒŒì¼:
```
./hubert_deberta_tar/
â”œâ”€â”€ train_shard_0000.tar
â”œâ”€â”€ train_shard_0001.tar
â”œâ”€â”€ train_shard_0002.tar
â”œâ”€â”€ train_shard_0003.tar
â”œâ”€â”€ .metadata_cache_train.pkl          # ğŸ†• ìºì‹œ íŒŒì¼
â”œâ”€â”€ eval_0_shard_0000.tar
â”œâ”€â”€ eval_0_shard_0001.tar
â”œâ”€â”€ .metadata_cache_eval_0.pkl         # ğŸ†• ìºì‹œ íŒŒì¼
â”œâ”€â”€ test_0_shard_0000.tar
â”œâ”€â”€ test_0_shard_0001.tar
â””â”€â”€ .metadata_cache_test_0.pkl         # ğŸ†• ìºì‹œ íŒŒì¼
```

### 3ë‹¨ê³„: ëª¨ë¸ ì‹¤í–‰

ìºì‹œ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìœ¼ë¯€ë¡œ, `eval.py` ì‹¤í–‰ ì‹œ ì²˜ìŒ ë¡œë“œê°€ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤:

```bash
python eval.py \
  --ckpt_path ./baseline_additional_loss/model_step300000.pt \
  --batch_size 256 \
  --num_workers 4 \
  --dataset_path ./hubert_deberta_tar \
  --use_tar True
```

---

## ì£¼ìš” ê°œì„ ì‚¬í•­

### âœ… ì¥ì 
1. **ìºì‹œ ìë™ ìƒì„±** - tar ìƒì„± ì‹œ ë©”íƒ€ë°ì´í„° ìºì‹œê°€ í•¨ê»˜ ìƒì„±ë¨
2. **ì²« ë¡œë“œ ì‹œê°„ ë‹¨ì¶•** - ìºì‹œê°€ ìˆìœ¼ë©´ ë©”íƒ€ë°ì´í„° ë¡œë“œê°€ ê±°ì˜ ì¦‰ì‹œ ì™„ë£Œ
3. **ì›í´ë¦­ ì²˜ë¦¬** - `create_tar_dataset.py`ì—ì„œ í† í¬ë‚˜ì´ì € ì˜µì…˜ë§Œ ì¶”ê°€í•˜ë©´ ë
4. **ì•ˆì •ì„±** - í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ ì‹œì—ë„ tar ìƒì„±ì€ ê³„ì†ë¨

### ğŸ“Š ì„±ëŠ¥ ë¹„êµ

**ì²« ì‹¤í–‰ ì‹œ (ìºì‹œ ì—†ìŒ):**
- ì´ì „: tar íŒŒì¼ì„ ì½ìœ¼ë©´ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ëŠë¦¼)
- ê°œì„ : tar ìƒì„± ë‹¨ê³„ì—ì„œ ì´ë¯¸ ìºì‹œ ìƒì„± ì™„ë£Œ (ë¹ ë¦„)

**ë‘ ë²ˆì§¸ ì´í›„ ì‹¤í–‰ (ìºì‹œ ìˆìŒ):**
- ìºì‹œ íŒŒì¼ë§Œ ë¡œë“œ (ë§¤ìš° ë¹ ë¦„)

---

## ì˜µì…˜ ì„¤ëª…

### `create_tar_dataset.py`
- `--tokenizer_model_name`: í† í¬ë‚˜ì´ì € ëª¨ë¸ (ì˜ˆ: `facebook/hubert-large-ls960-ft`)
  - ì§€ì •í•˜ë©´ ë©”íƒ€ë°ì´í„° ìºì‹œ ìë™ ìƒì„±
  - ë¯¸ì§€ì •í•˜ë©´ tarë§Œ ìƒì„± (ìºì‹œëŠ” ë‚˜ì¤‘ì— ì²« ì‹¤í–‰ ì‹œ ìƒì„±ë¨)

### `eval.py`
- `--use_tar`: `True` - tar íŒŒì¼ ì‚¬ìš© (ê¶Œì¥)
- `--dataset_path`: tar íŒŒì¼ ìœ„ì¹˜

---

## ì£¼ì˜ì‚¬í•­

1. **í† í¬ë‚˜ì´ì € ì„¤ì¹˜**: `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”
2. **ë©”ëª¨ë¦¬**: tar ìƒì„± ì¤‘ ë©”íƒ€ë°ì´í„° ì¶”ì¶œì— ë©”ëª¨ë¦¬ ì‚¬ìš©
3. **Shard ê°œìˆ˜**: ë„ˆë¬´ ë§ìœ¼ë©´ tar íŒŒì¼ì´ ì»¤ì§€ê³ , ë„ˆë¬´ ì ìœ¼ë©´ ê°œë³„ íŒŒì¼ì´ ì»¤ì§

---

## ìŠ¤í¬ë¦½íŠ¸ í•œë²ˆì— ì‹¤í–‰ (Bash)

```bash
#!/bin/bash

EXTRACT_FEAT_DIR="./hubert_deberta_cache_retrial"
TAR_OUTPUT_DIR="./hubert_deberta_tar"
TOKENIZER="facebook/hubert-large-ls960-ft"

# TAR ìƒì„± (ëª¨ë“  task)
for task in train eval_0 eval_1 test_0 test_1; do
  python create_tar_dataset.py \
    --source_dir $EXTRACT_FEAT_DIR \
    --output_dir $TAR_OUTPUT_DIR \
    --task $task \
    --num_shards 4 \
    --tokenizer_model_name $TOKENIZER   
done

echo "âœ… All tar files and cache files created!"
```
